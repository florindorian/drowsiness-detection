{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 22.0.2 from /home/rian/Programming/Python_progs/Cursos/F9_CV/C6-AnaliseFacial/fcv6-expressoes/.venv/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python==4.6.0.66 mediapipe==0.8.11 numpy==1.22.3 # DO CURSO     \n",
    "# %pip install -U opencv-python mediapipe numpy (O QUE USEI)\n",
    "\n",
    "# VERSÕES USADAS\n",
    "# opencv-contrib-python==4.11.0.86\n",
    "# opencv-python==4.11.0.86\n",
    "# numpy==1.26.4\n",
    "# mediapipe==0.10.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='limegreen'> Parte 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743270536.686741   30440 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743270536.687724  125862 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) Xe Graphics (TGL GT2)\n",
      "W0000 00:00:1743270536.691159  125852 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743270536.705926  125855 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "\n",
    "# (nível minímo de confiança para detectar uma face, mínimo de confiança para detectar os pontos da face)\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5, refine_landmarks=True) as facemesh:\n",
    "    while cap.isOpened():\n",
    "        tecla = cv2.waitKey(1)\n",
    "        if tecla == 27: # Esc\n",
    "            break\n",
    "\n",
    "\n",
    "        sucesso, frame = cap.read()\n",
    "        if not sucesso:\n",
    "            print('Ignorando o frame vazio da camera')\n",
    "            continue\n",
    "\n",
    "        # OpenCV por padrão entrega dados em BGR\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        saida_facemesh = facemesh.process(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            # Itera sobre cada ponto da face coletado\n",
    "            for face_landmarks in saida_facemesh.multi_face_landmarks:\n",
    "                # Frame (fundo), pontos da face, drawing options\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    face_landmarks, \n",
    "                    mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    # Blue, Green, Red (tonalidade tendendo pro Azul)\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(255,102,0),\n",
    "                        thickness=1, \n",
    "                        circle_radius=1\n",
    "                    ), \n",
    "                    # (tonalidade tendendo para o verde), espessura dos traços\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(102,204,0), \n",
    "                        thickness=1,\n",
    "                        circle_radius=1\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Caso ocorra algum erro, como por exemplo, a obstrução do rosto\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        cv2.imshow('Camera', frame) # Não colocar acento no nome da imagem!\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='limegreen'> Parte 2 - Mapeando Olhos</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saida_facemesh.multi_face_landmarks\n",
    "\n",
    "# [landmark {\n",
    "#    x: 0.55995214\n",
    "#    y: 0.746125638\n",
    "#    z: -0.0287616495\n",
    "#  }\n",
    "#  landmark {\n",
    "#    x: 0.556740046\n",
    "#    y: 0.683440685\n",
    "#    z: -0.0586610846\n",
    "#  }\n",
    "# ...\n",
    "# Ao todo, 468 pontos coletados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google._upb._message.RepeatedCompositeContainer"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(saida_facemesh.multi_face_landmarks) # list (de len=1)\n",
    "type(saida_facemesh.multi_face_landmarks[0])\n",
    "# mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList\n",
    "saida_facemesh.multi_face_landmarks[0].landmark\n",
    "# [x: 0.632665\n",
    "# y: 0.981715679\n",
    "# z: -0.0286340564\n",
    "# , x: 0.633395314\n",
    "# y: 0.919632852\n",
    "# z: -0.0591587462\n",
    "# , x: 0.630356848\n",
    "# ...\n",
    "type(saida_facemesh.multi_face_landmarks[0].landmark)\n",
    "# google._upb._message.RepeatedCompositeContainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for face_landmarks in saida_facemesh.multi_face_landmarks:\n",
    "    face = face_landmarks\n",
    "    for id_coord, coord_xyz in enumerate(face.landmark):\n",
    "        print(str(id_coord) + '\\n', coord_xyz)\n",
    "\n",
    "# 0\n",
    "#  x: 0.632665\n",
    "# y: 0.981715679\n",
    "# z: -0.0286340564\n",
    "\n",
    "# 1\n",
    "#  x: 0.633395314\n",
    "# y: 0.919632852\n",
    "# z: -0.0591587462\n",
    "\n",
    "# ...\n",
    "# 467\n",
    "#  x: 0.702854097\n",
    "# y: 0.758263469\n",
    "# z: 0.0227333456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr color='orange'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_olho_esq = [385, 380, 387, 373, 362, 263] # já estão em sequência\n",
    "p_olho_dir = [160, 144, 158, 153, 33, 133] # já estão em sequência\n",
    "p_olhos = p_olho_esq + p_olho_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_ear(face, p_olho_dir, p_olho_esq):\n",
    "    # Para evitar problemas como divisão por 0\n",
    "    try: \n",
    "        face = np.array([[coord.x, coord.y] for coord in face]) # cria uma lista com apenas 2 das dimensões coletadas pelo mediapipe\n",
    "        # Coleta apenas os parâmetros de face do olho esquerdo e direito\n",
    "        face_esq = face[p_olho_esq,:]\n",
    "        face_dir = face[p_olho_dir,:]\n",
    "\n",
    "        # np.linalg.norm: Calcula a distância euclidiana | os pontos já estão na ordem, mas deveria ser: (||p2 - p6|| + ||p3 - p5||)/(2 * ||p1 - p4||)\n",
    "        ear_esq = (np.linalg.norm(face_esq[0] - face_esq[1]) + np.linalg.norm(face_esq[2] - face_esq[3])) / (2 * np.linalg.norm(face_esq[0] - face_esq[1]))\n",
    "        ear_dir = (np.linalg.norm(face_dir[0] - face_dir[1]) + np.linalg.norm(face_dir[2] - face_dir[3])) / (2 * np.linalg.norm(face_dir[0] - face_dir[1]))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ear_esq = 0\n",
    "        ear_dir = 0\n",
    "    \n",
    "    media_ear = (ear_esq + ear_dir)/2\n",
    "    return media_ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_limiar = 1.005 # Entre 1.000 e 1.010 no meu caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743276634.637807   30440 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743276634.638485  166352 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) Xe Graphics (TGL GT2)\n",
      "W0000 00:00:1743276634.641055  166346 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743276634.650427  166348 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "\n",
    "dormindo = 0 # Não está dormindo\n",
    "\n",
    "\n",
    "# (nível minímo de confiança para detectar uma face, mínimo de confiança para detectar os pontos da face)\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5, refine_landmarks=True) as facemesh:\n",
    "    while cap.isOpened():\n",
    "        tecla = cv2.waitKey(1)\n",
    "        if tecla == 27: # Esc\n",
    "            break\n",
    "\n",
    "\n",
    "        sucesso, frame = cap.read()\n",
    "        if not sucesso:\n",
    "            print('Ignorando o frame vazio da camera')\n",
    "            continue\n",
    "\n",
    "        # 3ª var: FPS\n",
    "        comprimento, largura, _ = frame.shape # para o _normalized_to_pixel_coordenates()\n",
    "\n",
    "        # OpenCV por padrão entrega dados em BGR\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        saida_facemesh = facemesh.process(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            # Itera sobre cada ponto da face coletado\n",
    "            for face_landmarks in saida_facemesh.multi_face_landmarks:\n",
    "                # Frame (fundo), pontos da face, drawing options\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    face_landmarks, \n",
    "                    mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    # Blue, Green, Red (tonalidade tendendo pro Azul)\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(255,102,0),\n",
    "                        thickness=1, \n",
    "                        circle_radius=1\n",
    "                    ), \n",
    "                    # (tonalidade tendendo para o verde), espessura dos traços\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(102,204,0), \n",
    "                        thickness=1,\n",
    "                        circle_radius=1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                face = face_landmarks.landmark\n",
    "                for id_coord, coord_xyz in enumerate(face):\n",
    "                    # Verifica se as coordenadas são de um ponto com id pertencente à lista referente aos olhos\n",
    "                    if id_coord in p_olhos:\n",
    "                        # Faz a conversão de coordenadas normalizadas para pixels\n",
    "                        coord_cv = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                            coord_xyz.x, coord_xyz.y, largura, comprimento)\n",
    "                        cv2.circle(frame, coord_cv, radius=2, color=(255,0,0), thickness=-1)\n",
    "                        \n",
    "                ear = calculo_ear(face, p_olho_dir, p_olho_esq)\n",
    "                # Para ter um fundo e criar contraste para o texto\n",
    "                # Ponto inicial | Ponto final do retângulo | cor (cinza) | thickness=-1 (retângulo preenchido)\n",
    "                cv2.rectangle(\n",
    "                    frame, pt1=(0,1), pt2=(290, 140), color=(58,58,55), thickness=-1)\n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    text=f\"EAR: {round(ear, 2)}\", org=(1, 24),\n",
    "                    fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    fontScale=0.9, color=(255, 255, 255), thickness=2)\n",
    "                \n",
    "                if ear < ear_limiar:\n",
    "                    # dormindo já começa como 0, logo não haveria o problema da primeira inicialização de t_inicial quando dormindo = 1\n",
    "                    t_inicial = time.time() if dormindo == 0 else t_inicial\n",
    "                    dormindo = 1\n",
    "                if dormindo == 1 and ear >= ear_limiar:\n",
    "                    # enquanto estiver com o olho fechado e o \"ear\" não for maior que o limiar (o olho ainda não abriu), não entra nesse caso para definir que o olho de fato abriu\n",
    "                    dormindo = 0 # Não se está mais em descanso\n",
    "\n",
    "                t_final = time.time()\n",
    "                # Só conta o tempo se o olho estiver fechado (dormindo verdadeiro)\n",
    "                tempo = (t_final - t_inicial) if dormindo == 1 else 0.0\n",
    "                \n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    text=f\"Tempo: {round(tempo), 3}\", org=(1, 80),\n",
    "                    fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    fontScale=0.9, color=(255, 255, 255), thickness=2)\n",
    "                \n",
    "                if tempo>=1.5:\n",
    "                    cv2.rectangle(frame, pt1=(30, 400), pt2=(610, 452), color=(109, 233, 219), thickness=-1) # Preenche todo o retângulo\n",
    "                    cv2.putText(\n",
    "                        frame, \n",
    "                        text=f\"Muito tempo com olhos fechados!\", \n",
    "                        org=(80, 435), # cor do texto (mesmo cinza do retângulo)\n",
    "                        fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                        fontScale=0.85, \n",
    "                        color=(58,58,55), \n",
    "                        thickness=1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Caso ocorra algum erro, como por exemplo, a obstrução do rosto\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        cv2.imshow('Camera', frame) # Não colocar acento no nome da imagem!\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
