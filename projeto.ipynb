{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip 22.0.2 from /home/rian/Programming/Python_progs/Cursos/F9_CV/C6-AnaliseFacial/fcv6-expressoes/.venv/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python==4.6.0.66 mediapipe==0.8.11 numpy==1.22.3 # DO CURSO     \n",
    "# %pip install -U opencv-python mediapipe numpy (O QUE USEI)\n",
    "\n",
    "# VERSÕES USADAS\n",
    "# opencv-contrib-python==4.11.0.86\n",
    "# opencv-python==4.11.0.86\n",
    "# numpy==1.26.4\n",
    "# mediapipe==0.10.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='limegreen'> Parte 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743270536.686741   30440 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743270536.687724  125862 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) Xe Graphics (TGL GT2)\n",
      "W0000 00:00:1743270536.691159  125852 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743270536.705926  125855 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n",
      "'NoneType' object is not iterable\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "\n",
    "# (nível minímo de confiança para detectar uma face, mínimo de confiança para detectar os pontos da face)\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5, refine_landmarks=True) as facemesh:\n",
    "    while cap.isOpened():\n",
    "        tecla = cv2.waitKey(1)\n",
    "        if tecla == 27: # Esc\n",
    "            break\n",
    "\n",
    "\n",
    "        sucesso, frame = cap.read()\n",
    "        if not sucesso:\n",
    "            print('Ignorando o frame vazio da camera')\n",
    "            continue\n",
    "\n",
    "        # OpenCV por padrão entrega dados em BGR\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        saida_facemesh = facemesh.process(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            # Itera sobre cada ponto da face coletado\n",
    "            for face_landmarks in saida_facemesh.multi_face_landmarks:\n",
    "                # Frame (fundo), pontos da face, drawing options\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    face_landmarks, \n",
    "                    mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    # Blue, Green, Red (tonalidade tendendo pro Azul)\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(255,102,0),\n",
    "                        thickness=1, \n",
    "                        circle_radius=1\n",
    "                    ), \n",
    "                    # (tonalidade tendendo para o verde), espessura dos traços\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(102,204,0), \n",
    "                        thickness=1,\n",
    "                        circle_radius=1\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Caso ocorra algum erro, como por exemplo, a obstrução do rosto\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        cv2.imshow('Camera', frame) # Não colocar acento no nome da imagem!\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='limegreen'> Parte 2 - Mapeando Olhos</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saida_facemesh.multi_face_landmarks\n",
    "\n",
    "# [landmark {\n",
    "#    x: 0.55995214\n",
    "#    y: 0.746125638\n",
    "#    z: -0.0287616495\n",
    "#  }\n",
    "#  landmark {\n",
    "#    x: 0.556740046\n",
    "#    y: 0.683440685\n",
    "#    z: -0.0586610846\n",
    "#  }\n",
    "# ...\n",
    "# Ao todo, 468 pontos coletados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google._upb._message.RepeatedCompositeContainer"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(saida_facemesh.multi_face_landmarks) # list (de len=1)\n",
    "type(saida_facemesh.multi_face_landmarks[0])\n",
    "# mediapipe.framework.formats.landmark_pb2.NormalizedLandmarkList\n",
    "saida_facemesh.multi_face_landmarks[0].landmark\n",
    "# [x: 0.632665\n",
    "# y: 0.981715679\n",
    "# z: -0.0286340564\n",
    "# , x: 0.633395314\n",
    "# y: 0.919632852\n",
    "# z: -0.0591587462\n",
    "# , x: 0.630356848\n",
    "# ...\n",
    "type(saida_facemesh.multi_face_landmarks[0].landmark)\n",
    "# google._upb._message.RepeatedCompositeContainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for face_landmarks in saida_facemesh.multi_face_landmarks:\n",
    "    face = face_landmarks\n",
    "    for id_coord, coord_xyz in enumerate(face.landmark):\n",
    "        print(str(id_coord) + '\\n', coord_xyz)\n",
    "\n",
    "# 0\n",
    "#  x: 0.632665\n",
    "# y: 0.981715679\n",
    "# z: -0.0286340564\n",
    "\n",
    "# 1\n",
    "#  x: 0.633395314\n",
    "# y: 0.919632852\n",
    "# z: -0.0591587462\n",
    "\n",
    "# ...\n",
    "# 467\n",
    "#  x: 0.702854097\n",
    "# y: 0.758263469\n",
    "# z: 0.0227333456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr color='orange'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_olho_esq = [385, 380, 387, 373, 362, 263] # já estão em sequência\n",
    "p_olho_dir = [160, 144, 158, 153, 33, 133] # já estão em sequência\n",
    "p_olhos = p_olho_esq + p_olho_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_ear(face, p_olho_dir, p_olho_esq):\n",
    "    # Para evitar problemas como divisão por 0\n",
    "    try: \n",
    "        face = np.array([[coord.x, coord.y] for coord in face]) # cria uma lista com apenas 2 das dimensões coletadas pelo mediapipe\n",
    "        # Coleta apenas os parâmetros de face do olho esquerdo e direito\n",
    "        face_esq = face[p_olho_esq,:]\n",
    "        face_dir = face[p_olho_dir,:]\n",
    "\n",
    "        # np.linalg.norm: Calcula a distância euclidiana | os pontos já estão na ordem, mas deveria ser: (||p2 - p6|| + ||p3 - p5||)/(2 * ||p1 - p4||)\n",
    "        ear_esq = (np.linalg.norm(face_esq[0] - face_esq[1]) + np.linalg.norm(face_esq[2] - face_esq[3])) / (2 * np.linalg.norm(face_esq[0] - face_esq[1]))\n",
    "        ear_dir = (np.linalg.norm(face_dir[0] - face_dir[1]) + np.linalg.norm(face_dir[2] - face_dir[3])) / (2 * np.linalg.norm(face_dir[0] - face_dir[1]))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ear_esq = 0\n",
    "        ear_dir = 0\n",
    "    \n",
    "    media_ear = (ear_esq + ear_dir)/2\n",
    "    return media_ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_limiar = 1.005 # Entre 1.000 e 1.010 no meu caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743276834.813765   30440 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743276834.814396  167578 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) Xe Graphics (TGL GT2)\n",
      "W0000 00:00:1743276834.817085  167571 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743276834.828578  167568 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "\n",
    "dormindo = 0 # Não está dormindo\n",
    "\n",
    "\n",
    "# (nível minímo de confiança para detectar uma face, mínimo de confiança para detectar os pontos da face)\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5, refine_landmarks=True) as facemesh:\n",
    "    while cap.isOpened():\n",
    "        tecla = cv2.waitKey(1)\n",
    "        if tecla == 27: # Esc\n",
    "            break\n",
    "\n",
    "\n",
    "        sucesso, frame = cap.read()\n",
    "        if not sucesso:\n",
    "            print('Ignorando o frame vazio da camera')\n",
    "            continue\n",
    "\n",
    "        # 3ª var: FPS\n",
    "        comprimento, largura, _ = frame.shape # para o _normalized_to_pixel_coordenates()\n",
    "\n",
    "        # OpenCV por padrão entrega dados em BGR\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        saida_facemesh = facemesh.process(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            # Itera sobre cada ponto da face coletado\n",
    "            for face_landmarks in saida_facemesh.multi_face_landmarks:\n",
    "                # Frame (fundo), pontos da face, drawing options\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    face_landmarks, \n",
    "                    mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    # Blue, Green, Red (tonalidade tendendo pro Azul)\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(255,102,0),\n",
    "                        thickness=1, \n",
    "                        circle_radius=1\n",
    "                    ), \n",
    "                    # (tonalidade tendendo para o verde), espessura dos traços\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(102,204,0), \n",
    "                        thickness=1,\n",
    "                        circle_radius=1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                face = face_landmarks.landmark\n",
    "                for id_coord, coord_xyz in enumerate(face):\n",
    "                    # Verifica se as coordenadas são de um ponto com id pertencente à lista referente aos olhos\n",
    "                    if id_coord in p_olhos:\n",
    "                        # Faz a conversão de coordenadas normalizadas para pixels\n",
    "                        coord_cv = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                            coord_xyz.x, coord_xyz.y, largura, comprimento)\n",
    "                        cv2.circle(frame, coord_cv, radius=2, color=(255,0,0), thickness=-1)\n",
    "                        \n",
    "                ear = calculo_ear(face, p_olho_dir, p_olho_esq)\n",
    "                # Para ter um fundo e criar contraste para o texto\n",
    "                # Ponto inicial | Ponto final do retângulo | cor (cinza) | thickness=-1 (retângulo preenchido)\n",
    "                cv2.rectangle(\n",
    "                    frame, pt1=(0,1), pt2=(290, 140), color=(58,58,55), thickness=-1)\n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    text=f\"EAR: {round(ear, 2)}\", org=(1, 24),\n",
    "                    fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    fontScale=0.9, color=(255, 255, 255), thickness=2)\n",
    "                \n",
    "                if ear < ear_limiar:\n",
    "                    # dormindo já começa como 0, logo não haveria o problema da primeira inicialização de t_inicial quando dormindo = 1\n",
    "                    t_inicial = time.time() if dormindo == 0 else t_inicial\n",
    "                    dormindo = 1\n",
    "                if dormindo == 1 and ear >= ear_limiar:\n",
    "                    # enquanto estiver com o olho fechado e o \"ear\" não for maior que o limiar (o olho ainda não abriu), não entra nesse caso para definir que o olho de fato abriu\n",
    "                    dormindo = 0 # Não se está mais em descanso\n",
    "\n",
    "                t_final = time.time()\n",
    "                # Só conta o tempo se o olho estiver fechado (dormindo verdadeiro)\n",
    "                tempo = (t_final - t_inicial) if dormindo == 1 else 0.0\n",
    "                \n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    text=f\"Tempo: {round(tempo), 3}\", org=(1, 80),\n",
    "                    fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    fontScale=0.9, color=(255, 255, 255), thickness=2)\n",
    "                \n",
    "                if tempo>=1.5:\n",
    "                    cv2.rectangle(frame, pt1=(30, 400), pt2=(610, 452), color=(109, 233, 219), thickness=-1) # Preenche todo o retângulo\n",
    "                    cv2.putText(\n",
    "                        frame, \n",
    "                        text=f\"Muito tempo com olhos fechados!\", \n",
    "                        org=(80, 435), # cor do texto (mesmo cinza do retângulo)\n",
    "                        fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                        fontScale=0.85, \n",
    "                        color=(58,58,55), \n",
    "                        thickness=1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Caso ocorra algum erro, como por exemplo, a obstrução do rosto\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        cv2.imshow('Camera', frame) # Não colocar acento no nome da imagem!\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='limegreen'> Parte 3 - Mapeando a Boca</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_olho_esq = [385, 380, 387, 373, 362, 263] # já estão em sequência\n",
    "p_olho_dir = [160, 144, 158, 153, 33, 133] # já estão em sequência\n",
    "p_olhos = p_olho_esq + p_olho_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_ear(face, p_olho_dir, p_olho_esq):\n",
    "    # Para evitar problemas como divisão por 0\n",
    "    try: \n",
    "        face = np.array([[coord.x, coord.y] for coord in face]) # cria uma lista com apenas 2 das dimensões coletadas pelo mediapipe\n",
    "        # Coleta apenas os parâmetros de face do olho esquerdo e direito\n",
    "        face_esq = face[p_olho_esq,:]\n",
    "        face_dir = face[p_olho_dir,:]\n",
    "\n",
    "        # np.linalg.norm: Calcula a distância euclidiana | os pontos já estão na ordem, mas deveria ser: (||p2 - p6|| + ||p3 - p5||)/(2 * ||p1 - p4||)\n",
    "        ear_esq = (np.linalg.norm(face_esq[0] - face_esq[1]) + np.linalg.norm(face_esq[2] - face_esq[3])) / (2 * np.linalg.norm(face_esq[0] - face_esq[1]))\n",
    "        ear_dir = (np.linalg.norm(face_dir[0] - face_dir[1]) + np.linalg.norm(face_dir[2] - face_dir[3])) / (2 * np.linalg.norm(face_dir[0] - face_dir[1]))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ear_esq = 0\n",
    "        ear_dir = 0\n",
    "    \n",
    "    media_ear = (ear_esq + ear_dir)/2\n",
    "    return media_ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_boca = [82, 87, 13, 14, 312, 317, 78, 308]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_mar(face,p_boca):\n",
    "    # Ao abrir a boca ou sorrir, o MAR aumenta\n",
    "    try:\n",
    "        face = np.array([[coord.x, coord.y] for coord in face])\n",
    "        face_boca = face[p_boca,:]\n",
    "\n",
    "        mar = (np.linalg.norm(face_boca[0]-face_boca[1])+np.linalg.norm(face_boca[2]-face_boca[3])+np.linalg.norm(face_boca[4]-face_boca[5]))/(2*(np.linalg.norm(face_boca[6]-face_boca[7])))\n",
    "    except:\n",
    "        mar = 0.0\n",
    "\n",
    "    return mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_limiar = 1.005 # Entre 1.000 e 1.010 no meu caso\n",
    "mar_limiar = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743278995.306672   30440 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743278995.307753  182167 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) Xe Graphics (TGL GT2)\n",
      "W0000 00:00:1743278995.309565  182162 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743278995.322078  182160 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "\n",
    "dormindo = 0 # Não está dormindo\n",
    "\n",
    "\n",
    "# (nível minímo de confiança para detectar uma face, mínimo de confiança para detectar os pontos da face)\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5, refine_landmarks=True) as facemesh:\n",
    "    while cap.isOpened():\n",
    "        tecla = cv2.waitKey(1)\n",
    "        if tecla == 27: # Esc\n",
    "            break\n",
    "\n",
    "        sucesso, frame = cap.read()\n",
    "        if not sucesso:\n",
    "            print('Ignorando o frame vazio da camera')\n",
    "            continue\n",
    "\n",
    "        # 3ª var: FPS\n",
    "        comprimento, largura, _ = frame.shape # para o _normalized_to_pixel_coordenates()\n",
    "\n",
    "        # OpenCV por padrão entrega dados em BGR\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        saida_facemesh = facemesh.process(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            # Itera sobre cada ponto da face coletado\n",
    "            for face_landmarks in saida_facemesh.multi_face_landmarks:\n",
    "                # Frame (fundo), pontos da face, drawing options\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    face_landmarks, \n",
    "                    mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    # Blue, Green, Red (tonalidade tendendo pro Azul)\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(255,102,0),\n",
    "                        thickness=1, \n",
    "                        circle_radius=1\n",
    "                    ), \n",
    "                    # (tonalidade tendendo para o verde), espessura dos traços\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(102,204,0), \n",
    "                        thickness=1,\n",
    "                        circle_radius=1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                face = face_landmarks.landmark\n",
    "                for id_coord, coord_xyz in enumerate(face):\n",
    "                    # Verifica se as coordenadas são de um ponto com id pertencente à lista referente aos olhos\n",
    "                    if id_coord in p_olhos:\n",
    "                        # Faz a conversão de coordenadas normalizadas para pixels\n",
    "                        coord_cv = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                            coord_xyz.x, coord_xyz.y, largura, comprimento)\n",
    "                        cv2.circle(frame, coord_cv, radius=2, color=(255,0,0), thickness=-1)\n",
    "\n",
    "                    if id_coord in p_boca:\n",
    "                        # Faz a conversão de coordenadas normalizadas para pixels\n",
    "                        coord_cv = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                            coord_xyz.x, coord_xyz.y, largura, comprimento)\n",
    "                        cv2.circle(frame, coord_cv, radius=2, color=(255,0,0), thickness=-1)\n",
    "                        \n",
    "                ear = calculo_ear(face, p_olho_dir, p_olho_esq)\n",
    "                # Para ter um fundo e criar contraste para o texto\n",
    "                # Ponto inicial | Ponto final do retângulo | cor (cinza) | thickness=-1 (retângulo preenchido)\n",
    "                cv2.rectangle(\n",
    "                    frame, pt1=(0,1), pt2=(290, 140), color=(58,58,55), thickness=-1)\n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    text=f\"EAR: {round(ear, 2)}\", org=(1, 24),\n",
    "                    fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    fontScale=0.9, color=(255, 255, 255), thickness=2)\n",
    "                \n",
    "                mar = calculo_mar(face,p_boca)\n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    f\"MAR: {round(mar, 2)} {'Aberto' if mar>=mar_limiar else 'Fechado'}\", \n",
    "                    (1, 50),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    0.9, (255, 255, 255), 2)\n",
    "                \n",
    "                if ear < ear_limiar and mar < mar_limiar:\n",
    "                    # dormindo já começa como 0, logo não haveria o problema da primeira inicialização de t_inicial quando dormindo = 1\n",
    "                    t_inicial = time.time() if dormindo == 0 else t_inicial\n",
    "                    dormindo = 1\n",
    "                if (dormindo == 1 and ear >= ear_limiar) or mar >= mar_limiar:\n",
    "                    # enquanto estiver com o olho fechado e o \"ear\" não for maior que o limiar (o olho ainda não abriu), não entra nesse caso para definir que o olho de fato abriu\n",
    "                    # se a boca estiver aberta, significa que a pessoa está acordada\n",
    "                    dormindo = 0 # Não se está mais em descanso\n",
    "\n",
    "                t_final = time.time()\n",
    "                # Só conta o tempo se o olho estiver fechado (dormindo verdadeiro)\n",
    "                tempo = (t_final - t_inicial) if dormindo == 1 else 0.0\n",
    "                \n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    text=f\"Tempo: {round(tempo, 3)}\", org=(1, 80),\n",
    "                    fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    fontScale=0.9, color=(255, 255, 255), thickness=2)\n",
    "                \n",
    "                if tempo>=1.5:\n",
    "                    cv2.rectangle(frame, pt1=(30, 400), pt2=(610, 452), color=(109, 233, 219), thickness=-1) # Preenche todo o retângulo\n",
    "                    cv2.putText(\n",
    "                        frame, \n",
    "                        text=f\"Muito tempo com olhos fechados!\", \n",
    "                        org=(80, 435), # cor do texto (mesmo cinza do retângulo)\n",
    "                        fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                        fontScale=0.85, \n",
    "                        color=(58,58,55), \n",
    "                        thickness=1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Caso ocorra algum erro, como por exemplo, a obstrução do rosto\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        cv2.imshow('Camera', frame) # Não colocar acento no nome da imagem!\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='limegreen'> Parte 4 - Detecção de Sono</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## <font color='limegreen'> Parte 3 - Mapeando a Boca</font>\n",
    "p_olho_esq = [385, 380, 387, 373, 362, 263] # já estão em sequência\n",
    "p_olho_dir = [160, 144, 158, 153, 33, 133] # já estão em sequência\n",
    "p_olhos = p_olho_esq + p_olho_dir\n",
    "\n",
    "p_boca = [82, 87, 13, 14, 312, 317, 78, 308]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_ear(face, p_olho_dir, p_olho_esq):\n",
    "    # Para evitar problemas como divisão por 0\n",
    "    try: \n",
    "        face = np.array([[coord.x, coord.y] for coord in face]) # cria uma lista com apenas 2 das dimensões coletadas pelo mediapipe\n",
    "        # Coleta apenas os parâmetros de face do olho esquerdo e direito\n",
    "        face_esq = face[p_olho_esq,:]\n",
    "        face_dir = face[p_olho_dir,:]\n",
    "\n",
    "        # np.linalg.norm: Calcula a distância euclidiana | os pontos já estão na ordem, mas deveria ser: (||p2 - p6|| + ||p3 - p5||)/(2 * ||p1 - p4||)\n",
    "        ear_esq = (np.linalg.norm(face_esq[0] - face_esq[1]) + np.linalg.norm(face_esq[2] - face_esq[3])) / (2 * np.linalg.norm(face_esq[0] - face_esq[1]))\n",
    "        ear_dir = (np.linalg.norm(face_dir[0] - face_dir[1]) + np.linalg.norm(face_dir[2] - face_dir[3])) / (2 * np.linalg.norm(face_dir[0] - face_dir[1]))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        ear_esq = 0\n",
    "        ear_dir = 0\n",
    "    \n",
    "    media_ear = (ear_esq + ear_dir)/2\n",
    "    return media_ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculo_mar(face,p_boca):\n",
    "    # Ao abrir a boca ou sorrir, o MAR aumenta\n",
    "    try:\n",
    "        face = np.array([[coord.x, coord.y] for coord in face])\n",
    "        face_boca = face[p_boca,:]\n",
    "\n",
    "        mar = (np.linalg.norm(face_boca[0]-face_boca[1])+np.linalg.norm(face_boca[2]-face_boca[3])+np.linalg.norm(face_boca[4]-face_boca[5]))/(2*(np.linalg.norm(face_boca[6]-face_boca[7])))\n",
    "    except:\n",
    "        mar = 0.0\n",
    "\n",
    "    return mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_limiar = 1.018 # Entre 1.000 e 1.010 no meu caso\n",
    "mar_limiar = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743340666.715349   19683 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1743340666.715984   23587 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: Mesa Intel(R) Xe Graphics (TGL GT2)\n",
      "W0000 00:00:1743340666.719430   23578 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1743340666.731575   23581 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "\n",
    "\n",
    "dormindo = 0 # Não está dormindo\n",
    "contagem_piscadas = 0\n",
    "t_piscadas = time.time()\n",
    "c_tempo = 0 # Contador temporário de tempo\n",
    "contagem_temporaria = 0 # Altera conforme a última quantidade de piscadas antes de ser dado um segundo\n",
    "contagem_lista = []\n",
    "\n",
    "\n",
    "# (nível minímo de confiança para detectar uma face, mínimo de confiança para detectar os pontos da face)\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5, refine_landmarks=True) as facemesh:\n",
    "    while cap.isOpened():\n",
    "        tecla = cv2.waitKey(1)\n",
    "        if tecla == 27: # Esc\n",
    "            break\n",
    "\n",
    "        sucesso, frame = cap.read()\n",
    "        if not sucesso:\n",
    "            print('Ignorando o frame vazio da camera')\n",
    "            continue\n",
    "\n",
    "        # 3ª var: FPS\n",
    "        comprimento, largura, _ = frame.shape # para o _normalized_to_pixel_coordenates()\n",
    "\n",
    "        # OpenCV por padrão entrega dados em BGR\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        saida_facemesh = facemesh.process(frame)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            # Itera sobre cada ponto da face coletado\n",
    "            for face_landmarks in saida_facemesh.multi_face_landmarks:\n",
    "                # Frame (fundo), pontos da face, drawing options\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, \n",
    "                    face_landmarks, \n",
    "                    mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    # Blue, Green, Red (tonalidade tendendo pro Azul)\n",
    "                    landmark_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(255,102,0),\n",
    "                        thickness=1, \n",
    "                        circle_radius=1\n",
    "                    ), \n",
    "                    # (tonalidade tendendo para o verde), espessura dos traços\n",
    "                    connection_drawing_spec=mp_drawing.DrawingSpec(\n",
    "                        color=(102,204,0), \n",
    "                        thickness=1,\n",
    "                        circle_radius=1\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                face = face_landmarks.landmark\n",
    "                for id_coord, coord_xyz in enumerate(face):\n",
    "                    # Verifica se as coordenadas são de um ponto com id pertencente à lista referente aos olhos\n",
    "                    if id_coord in p_olhos:\n",
    "                        # Faz a conversão de coordenadas normalizadas para pixels\n",
    "                        coord_cv = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                            coord_xyz.x, coord_xyz.y, largura, comprimento)\n",
    "                        cv2.circle(frame, coord_cv, radius=2, color=(255,0,0), thickness=-1)\n",
    "\n",
    "                    if id_coord in p_boca:\n",
    "                        # Faz a conversão de coordenadas normalizadas para pixels\n",
    "                        coord_cv = mp_drawing._normalized_to_pixel_coordinates(\n",
    "                            coord_xyz.x, coord_xyz.y, largura, comprimento)\n",
    "                        cv2.circle(frame, coord_cv, radius=2, color=(255,0,0), thickness=-1)\n",
    "                        \n",
    "                ear = calculo_ear(face, p_olho_dir, p_olho_esq)\n",
    "                # Para ter um fundo e criar contraste para o texto\n",
    "                # Ponto inicial | Ponto final do retângulo | cor (cinza) | thickness=-1 (retângulo preenchido)\n",
    "                cv2.rectangle(\n",
    "                    frame, pt1=(0,1), pt2=(290, 140), color=(58,58,55), thickness=-1)\n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    text=f\"EAR: {round(ear, 2)}\", org=(1, 24),\n",
    "                    fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    fontScale=0.9, color=(255, 255, 255), thickness=2)\n",
    "                \n",
    "                mar = calculo_mar(face,p_boca)\n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    f\"MAR: {round(mar, 2)} {'Aberto' if mar>=mar_limiar else 'Fechado'}\", \n",
    "                    (1, 50),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    0.9, (255, 255, 255), 2)\n",
    "                \n",
    "                if ear < ear_limiar and mar < mar_limiar: # Detecta a piscada\n",
    "                    # dormindo já começa como 0, logo não haveria o problema da primeira inicialização de t_inicial quando dormindo = 1\n",
    "                    t_inicial = time.time() if dormindo == 0 else t_inicial\n",
    "                    # o if-ternário evita a contagem infinita quando se está dormindo)\n",
    "                    contagem_piscadas = (contagem_piscadas + 1 if dormindo == 0 else contagem_piscadas)\n",
    "                    dormindo = 1\n",
    "                if (dormindo == 1 and ear >= ear_limiar) or mar >= mar_limiar:\n",
    "                    # enquanto estiver com o olho fechado e o \"ear\" não for maior que o limiar (o olho ainda não abriu), não entra nesse caso para definir que o olho de fato abriu\n",
    "                    # se a boca estiver aberta, significa que a pessoa está acordada\n",
    "                    dormindo = 0 # Não se está mais em descanso\n",
    "\n",
    "                t_final = time.time()\n",
    "                tempo_decorrido = t_final - t_piscadas\n",
    "\n",
    "                # Verifica se já se passou 1 segundo\n",
    "                if tempo_decorrido >= c_tempo + 1:\n",
    "                    c_tempo = tempo_decorrido # Atualiza o contador de segundos\n",
    "                    piscadas_ps = contagem_piscadas - contagem_temporaria # Recebe as piscadas apenas do último intervalo de 1 segundo\n",
    "                    contagem_temporaria = contagem_piscadas # Limite (inferior) passa a ser o Limite (superior) antigo\n",
    "                    contagem_lista.append(piscadas_ps)\n",
    "                    # Importante para não acessar índices inválidos enquanto a lista não tem 60 elementos. Após ela ultrapassar essa quantidade, conterá apenas os últimos 60 elementos\n",
    "                    contagem_lista = contagem_lista if (len(contagem_lista) <= 60) else contagem_lista[-60:] # contagem_lista[-60:] -> Seleciona apenas os últimos 60 elementos\n",
    "                \n",
    "                # Somatório das piscadas do último minuto\n",
    "                piscadas_pm = 15 if (tempo_decorrido <= 60) else sum(contagem_lista)\n",
    "                # O if-ternário é para não prejudicar a análise enquanto ainda não completar 1 novo minuto\n",
    "\n",
    "                # Só conta o tempo se o olho estiver fechado (dormindo verdadeiro)\n",
    "                tempo = (t_final - t_inicial) if dormindo == 1 else 0.0\n",
    "\n",
    "\n",
    "                cv2.putText(frame, f\"Piscadas: {contagem_piscadas}\", (1, 120),\n",
    "                                                cv2.FONT_HERSHEY_DUPLEX,\n",
    "                                                0.9, (109, 233, 219), 2)\n",
    "\n",
    "                cv2.putText(\n",
    "                    frame, \n",
    "                    text=f\"Tempo: {round(tempo, 3)}\", org=(1, 80),\n",
    "                    fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    fontScale=0.9, color=(255, 255, 255), thickness=2)\n",
    "                \n",
    "                if piscadas_pm < 10 or tempo>=1.5:\n",
    "                    cv2.rectangle(frame, pt1=(30+340, 400+230), pt2=(610+340, 452+230), color=(109, 233, 219), thickness=-1) # Preenche todo o retângulo\n",
    "                    # cv2.putText(\n",
    "                    #     frame, \n",
    "                    #     text=f\"Muito tempo com olhos fechados!\", \n",
    "                    #     org=(80, 435), # cor do texto (mesmo cinza do retângulo)\n",
    "                    #     fontFace=cv2.FONT_HERSHEY_DUPLEX,\n",
    "                    #     fontScale=0.85, \n",
    "                    #     color=(58,58,55), \n",
    "                    #     thickness=1)\n",
    "\n",
    "                    cv2.putText(frame, f\"Pode ser que voce esteja com sono,\", (400, 650),\n",
    "                                        cv2.FONT_HERSHEY_DUPLEX, \n",
    "                                        0.85, (58,58,55), 1)\n",
    "                    cv2.putText(frame, f\"considere descansar.\", (520, 680),\n",
    "                                                            cv2.FONT_HERSHEY_DUPLEX, \n",
    "                                                            0.85, (58,58,55), 1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Caso ocorra algum erro, como por exemplo, a obstrução do rosto\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        cv2.imshow('Camera', frame) # Não colocar acento no nome da imagem!\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
